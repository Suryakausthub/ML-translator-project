{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.15","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"tpu1vmV38","dataSources":[{"sourceId":10027078,"sourceType":"datasetVersion","datasetId":6175047}],"dockerImageVersionId":30788,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-11-27T13:54:01.644744Z","iopub.execute_input":"2024-11-27T13:54:01.645118Z","iopub.status.idle":"2024-11-27T13:54:01.650187Z","shell.execute_reply.started":"2024-11-27T13:54:01.645083Z","shell.execute_reply":"2024-11-27T13:54:01.649337Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"# Install openpyxl to read Excel files\n!pip install openpyxl\n\n# Step 1: Import Necessary Libraries\n\nimport numpy as np\nimport pandas as pd\nimport os\nimport string\nfrom string import digits\nimport re\nfrom sklearn.model_selection import train_test_split\nimport tensorflow as tf\n\nprint(\"Libraries imported successfully.\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Step 2: Set up TPU\n\ntry:\n    # Detect TPU\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()  # TPU detection\n    print('Running on TPU ', tpu.master())\nexcept ValueError:\n    tpu = None\n\nif tpu:\n    # Connect to TPU cluster\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    # Create TPU strategy\n    strategy = tf.distribute.TPUStrategy(tpu)\nelse:\n    strategy = tf.distribute.get_strategy()  # Default strategy\n\nprint(\"REPLICAS: \", strategy.num_replicas_in_sync)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Step 3: Load the Dataset\n\n# List files in the input directory\nprint(\"Files in the input directory:\")\nprint(os.listdir(\"../input\"))\n\n# Adjust the file path according to your dataset\n# Replace 'hinditotelugu' and 'FINAL_ML (1).xlsx' with actual names if different\ndata = pd.read_excel(\"/kaggle/input/mlfinal/FINAL_ML (1).xlsx\", names=['HINDI', 'TELUGU'], header=None, skiprows=1)\n\n# Display the first few rows\nprint(\"\\nFirst few rows of the dataset:\")\nprint(data.head())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Step 4: Preprocess the Data\n\n# Check for missing values\nprint(\"\\nNumber of missing values in HINDI column:\", data['HINDI'].isnull().sum())\nprint(\"Number of missing values in TELUGU column:\", data['TELUGU'].isnull().sum())\n\n# Drop rows with missing values\ndata = data.dropna(subset=['HINDI', 'TELUGU'])\nprint(\"After dropping missing values, data shape:\", data.shape)\n\n# Remove empty strings\ndata = data[data['HINDI'].str.strip().astype(bool)]\ndata = data[data['TELUGU'].str.strip().astype(bool)]\ndata.reset_index(drop=True, inplace=True)\n\n# Convert all entries to strings\ndata['HINDI'] = data['HINDI'].astype(str)\ndata['TELUGU'] = data['TELUGU'].astype(str)\n\n# Lowercase all characters\ndata['HINDI'] = data['HINDI'].apply(lambda x: x.lower())\ndata['TELUGU'] = data['TELUGU'].apply(lambda x: x.lower())\n\n# Remove quotes\ndata['HINDI'] = data['HINDI'].apply(lambda x: x.replace(\"'\", ''))\ndata['TELUGU'] = data['TELUGU'].apply(lambda x: x.replace(\"'\", ''))\n\n# Remove punctuation\nexclude = set(string.punctuation)\ndata['HINDI'] = data['HINDI'].apply(lambda x: ''.join(ch for ch in x if ch not in exclude))\ndata['TELUGU'] = data['TELUGU'].apply(lambda x: ''.join(ch for ch in x if ch not in exclude))\n\n# Remove digits\nremove_digits = str.maketrans('', '', digits)\ndata['HINDI'] = data['HINDI'].apply(lambda x: x.translate(remove_digits))\ndata['TELUGU'] = data['TELUGU'].apply(lambda x: x.translate(remove_digits))\n\n# Remove extra spaces\ndata['HINDI'] = data['HINDI'].apply(lambda x: x.strip())\ndata['TELUGU'] = data['TELUGU'].apply(lambda x: x.strip())\ndata['HINDI'] = data['HINDI'].apply(lambda x: re.sub(\" +\", \" \", x))\ndata['TELUGU'] = data['TELUGU'].apply(lambda x: re.sub(\" +\", \" \", x))\n\n# Add start and end tokens to target sequences\ndata['TELUGU'] = data['TELUGU'].apply(lambda x: 'START_ ' + x + ' _END')\n\n# Display the preprocessed data\nprint(\"\\nPreprocessed Data:\")\nprint(data.head())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Step 5: Create Vocabulary Dictionaries\n\nall_hindi_words = set()\nfor hin in data['HINDI']:\n    for word in hin.split():\n        if word not in all_hindi_words:\n            all_hindi_words.add(word)\n\nall_telugu_words = set()\nfor tel in data['TELUGU']:\n    for word in tel.split():\n        if word not in all_telugu_words:\n            all_telugu_words.add(word)\n\nprint(\"\\nTotal unique Hindi words:\", len(all_hindi_words))\nprint(\"Total unique Telugu words:\", len(all_telugu_words))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Step 6: Calculate Sentence Lengths and Filter Sentences\n\ndata['length_hindi_sentence'] = data['HINDI'].apply(lambda x: len(x.split(\" \")))\ndata['length_telugu_sentence'] = data['TELUGU'].apply(lambda x: len(x.split(\" \")))\n\nprint(\"\\nSentence length statistics:\")\nprint(data[['length_hindi_sentence', 'length_telugu_sentence']].describe())\n\n# Set maximum sentence lengths (adjust as needed)\nmax_length_src = 20  # Hindi sentences\nmax_length_tar = 20  # Telugu sentences\n\n# Filter sentences based on length\ndata = data[data['length_hindi_sentence'] <= max_length_src]\ndata = data[data['length_telugu_sentence'] <= max_length_tar]\ndata.reset_index(drop=True, inplace=True)\n\nprint(\"\\nData after filtering based on sentence lengths:\")\nprint(data.head())\nprint(\"Total samples after filtering:\", data.shape[0])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Step 7: Prepare Data for Training\n\n# Sort the unique words to maintain consistency\ninput_words = sorted(list(all_hindi_words))\ntarget_words = sorted(list(all_telugu_words))\n\n# Define the number of unique tokens (+1 for padding)\nnum_encoder_tokens = len(input_words) + 1  # +1 for padding token\nnum_decoder_tokens = len(target_words) + 1  # +1 for padding token\n\n# Create word-to-index and index-to-word dictionaries for Hindi\ninput_token_index = dict([(word, i + 1) for i, word in enumerate(input_words)])\nreverse_input_token_index = dict((i, word) for word, i in input_token_index.items())\n\n# Create word-to-index and index-to-word dictionaries for Telugu\ntarget_token_index = dict([(word, i + 1) for i, word in enumerate(target_words)])\nreverse_target_token_index = dict((i, word) for word, i in target_token_index.items())\n\nprint(\"\\nNumber of unique input tokens (Hindi):\", num_encoder_tokens)\nprint(\"Number of unique output tokens (Telugu):\", num_decoder_tokens)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Step 8: Prepare Sequences and Split the Data\n\n# Prepare input sequences\nencoder_input_data = []\nfor input_text in data['HINDI']:\n    encoder_input_data.append([input_token_index.get(word, 0) for word in input_text.split()])\n\n# Prepare decoder input and output sequences\ndecoder_input_data = []\ndecoder_target_data = []\nfor target_text in data['TELUGU']:\n    target_words = target_text.split()\n    decoder_input_data.append([target_token_index.get(word, 0) for word in target_words[:-1]])  # exclude _END token\n    decoder_target_data.append([target_token_index.get(word, 0) for word in target_words[1:]])  # exclude START_ token\n\n# Pad sequences\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\n\nencoder_input_data = pad_sequences(encoder_input_data, maxlen=max_length_src, padding='post')\ndecoder_input_data = pad_sequences(decoder_input_data, maxlen=max_length_tar, padding='post')\ndecoder_target_data = pad_sequences(decoder_target_data, maxlen=max_length_tar, padding='post')\n\n# Convert decoder_target_data to one-hot vectors\ndecoder_target_data_onehot = tf.keras.utils.to_categorical(decoder_target_data, num_classes=num_decoder_tokens)\n\nprint(\"\\nData prepared for training.\")\n\n# Split the data into training and test sets\nX_train_enc, X_test_enc, X_train_dec_inp, X_test_dec_inp, y_train, y_test = train_test_split(\n    encoder_input_data, decoder_input_data, decoder_target_data_onehot, test_size=0.2, random_state=42)\n\nprint(\"\\nTraining and test data sizes:\")\nprint(\"Training data:\", X_train_enc.shape)\nprint(\"Test data:\", X_test_enc.shape)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Step 9: Build the Encoder-Decoder Model\n\nlatent_dim = 256  # Adjust as needed\n\nwith strategy.scope():\n    # Encoder\n    encoder_inputs = tf.keras.layers.Input(shape=(None,), name='encoder_inputs')\n    enc_emb = tf.keras.layers.Embedding(num_encoder_tokens, latent_dim, mask_zero=True, name='encoder_embedding')(encoder_inputs)\n    encoder_outputs, state_h, state_c = tf.keras.layers.LSTM(latent_dim, return_state=True, name='encoder_lstm')(enc_emb)\n    encoder_states = [state_h, state_c]\n\n    # Decoder\n    decoder_inputs = tf.keras.layers.Input(shape=(None,), name='decoder_inputs')\n    dec_emb_layer = tf.keras.layers.Embedding(num_decoder_tokens, latent_dim, mask_zero=True, name='decoder_embedding')\n    dec_emb = dec_emb_layer(decoder_inputs)\n    decoder_lstm = tf.keras.layers.LSTM(latent_dim, return_sequences=True, return_state=True, name='decoder_lstm')\n    decoder_outputs, _, _ = decoder_lstm(dec_emb, initial_state=encoder_states)\n    decoder_dense = tf.keras.layers.Dense(num_decoder_tokens, activation='softmax', name='decoder_dense')\n    decoder_outputs = decoder_dense(decoder_outputs)\n\n    # Define the model\n    model = tf.keras.models.Model([encoder_inputs, decoder_inputs], decoder_outputs)\n\n    # Compile the model\n    model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])\n\nprint(\"\\nModel Summary:\")\nmodel.summary()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Step 10: Train the Model\n\nbatch_size = 64  # Adjust as needed (should be a multiple of 8 for TPU)\nepochs = 12 # Adjust as needed\n\n# Since we are using numpy arrays, we can use model.fit directly\nhistory = model.fit([X_train_enc, X_train_dec_inp], y_train,\n                    batch_size=batch_size,\n                    epochs=epochs,\n                    validation_data=([X_test_enc, X_test_dec_inp], y_test))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Step 12: Build the Inference Model\n\nwith strategy.scope():\n    # Encoder inference model\n    encoder_model = tf.keras.models.Model(encoder_inputs, encoder_states)\n\n    # Decoder inference model\n    decoder_state_input_h = tf.keras.layers.Input(shape=(latent_dim,), name='decoder_state_input_h')\n    decoder_state_input_c = tf.keras.layers.Input(shape=(latent_dim,), name='decoder_state_input_c')\n    decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n\n    dec_emb2 = dec_emb_layer(decoder_inputs)\n\n    decoder_outputs2, state_h2, state_c2 = decoder_lstm(dec_emb2, initial_state=decoder_states_inputs)\n    decoder_states2 = [state_h2, state_c2]\n    decoder_outputs2 = decoder_dense(decoder_outputs2)\n\n    decoder_model = tf.keras.models.Model(\n        [decoder_inputs] + decoder_states_inputs,\n        [decoder_outputs2] + decoder_states2)\n\nprint(\"\\nInference models are ready.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Step 13: Define a Function to Decode Sequences\n\ndef decode_sequence(input_seq):\n    # Encode the input as state vectors.\n    states_value = encoder_model.predict(input_seq)\n\n    # Generate empty target sequence of length 1 with only the start token.\n    target_seq = np.zeros((1, 1))\n    target_seq[0, 0] = target_token_index.get('START_', 0)\n\n    # Sampling loop for a batch of sequences\n    stop_condition = False\n    decoded_sentence = ''\n    while not stop_condition:\n        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n\n        # Sample a token\n        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n        sampled_word = reverse_target_token_index.get(sampled_token_index, '')\n\n        if sampled_word != '_END':\n            decoded_sentence += ' ' + sampled_word\n\n        # Exit condition: either hit max length or find stop token.\n        if (sampled_word == '_END' or len(decoded_sentence.split()) > max_length_tar):\n            stop_condition = True\n\n        # Update the target sequence (of length 1).\n        target_seq = np.zeros((1, 1))\n        target_seq[0, 0] = sampled_token_index\n\n        # Update states\n        states_value = [h, c]\n\n    return decoded_sentence.strip()\n\nprint(\"\\nSequence decoding function is ready.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Step 14: Test the Model with Sample Sentences\n\nfor seq_index in range(5):\n    # Get the input sentence\n    input_seq = X_test_enc[seq_index: seq_index + 1]\n\n    # Decode the sequence to get the translated sentence\n    decoded_sentence = decode_sequence(input_seq)\n\n    # Get the actual TELUGU translation (remove START_ and _END tokens)\n    actual_translation = data['TELUGU'][X_train_enc.shape[0] + seq_index][7:-5]  # Adjust indices if necessary\n\n    print(f\"\\nInput Hindi sentence: {data['HINDI'][X_train_enc.shape[0] + seq_index]}\")\n    print(f\"Predicted sentence: {actual_translation}\")\n   \n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}